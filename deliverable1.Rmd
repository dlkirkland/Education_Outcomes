---
title: "Education Outcomes"
output:
  html_document:
    df_print: paged
name: Daniel Kirkland
---

In order to ask interesting questions about the educational outcomes of students attending college in the United States, we will explore a dataset called 'College Scorecard'.  This dataset as been sourced from collegescorecard.ed.gov/data/.  Per the website, the dataset was 

    "...designed to increase transparency, putting the 
    power in the hands of the public - from those  
    choosing colleges to those improving colege quality - to 
    see how well different schools are serving their students." 
    
The goal of this data science project is to use demographics to identify potential patterns or factors which are found among those who do and do not achieve success when pursing higher education.  

First, we must import the data in .csv format. As there exists historical data in separate .csv files for each year between 1996 - 2018, inclusive, a dataset for each year of interest will be imported to that more data will be available for analysis.  

    Steps: > import the 'tidyverse' library
           > create variables and store imported datasets 


```{r results = 'hide'}
install.packages("tidyverse")  
library(tidyverse)

library(dplyr)

#Read in, as tibbles, datasets for all calendar years 1996 - 2017.
CY17 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2017_18_PP.csv", header= T, na.strings= c("NULL")))
CY16 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2016_17_PP.csv", header= T, na.strings= c("NULL")))
CY15 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2015_16_PP.csv", header= T, na.strings= c("NULL")))
CY14 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2014_15_PP.csv", header= T, na.strings= c("NULL")))
CY13 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2013_14_PP.csv", header= T, na.strings= c("NULL")))
CY12 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2012_13_PP.csv", header= T, na.strings= c("NULL")))
CY11 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2011_12_PP.csv", header= T, na.strings= c("NULL")))
CY10 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2010_11_PP.csv", header= T, na.strings= c("NULL")))
CY09 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2009_10_PP.csv", header= T, na.strings= c("NULL")))
CY08 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2008_09_PP.csv", header= T, na.strings= c("NULL")))
CY07 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2007_08_PP.csv", header= T, na.strings= c("NULL")))
CY06 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2006_07_PP.csv", header= T, na.strings= c("NULL")))
CY05 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2005_06_PP.csv", header= T, na.strings= c("NULL")))
CY04 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2004_05_PP.csv", header= T, na.strings= c("NULL")))
CY03 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2003_04_PP.csv", header= T, na.strings= c("NULL")))
CY02 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2002_03_PP.csv", header= T, na.strings= c("NULL")))
CY01 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2001_02_PP.csv", header= T, na.strings= c("NULL")))
CY00 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED2000_01_PP.csv", header= T, na.strings= c("NULL")))
CY99 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED1999_00_PP.csv", header= T, na.strings= c("NULL")))
CY98 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED1998_99_PP.csv", header= T, na.strings= c("NULL")))
CY97 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED1997_98_PP.csv", header= T, na.strings= c("NULL")))
CY96 <- as_tibble(read.csv("CollegeScorecard_Raw_Data/MERGED1996_97_PP.csv", header= T, na.strings= c("NULL")))

```
The number of variables/columns in each dataset total 1,978, therefore, there are too many to list here.  But a data dictionary is included which describes the variables ('data dictionary' sheet), as well as the years for which values have been collected for each of the variables ('cohort' sheet). 

The datasets for each year from 1996 - 2018 (inclusive) all have the same number of columns/variables, but different
number of rows/observations.  Therefore, exluding CY98--which contains only 1,951 observations--the CY with the 
lowest of observations will be selected for comparison against all other CY datasets. This comparison dataset, 
CY99 with 6,466 observations, more closely matches the observation count of the remaining datasets, which is between
6,478 and 7804.  Since CY99 has less observations than subsequent CY datasets,  we can have AT MOST 6,466 observations
spanning across all CY datasets. We want to compare values for the same observations over all calendar years, so we 
must identify which observations appear in all datasets.  We must evaluate each dataset to determine which observations are common among the datasets. Once the most common observations among datasets have been found, the datasets will be merged together. 

```{r}
#First, we want to see which observations in CY99 are not found in CY17. 
#Let's identify the primary key in a few datasets, which should not contain any duplicates. 
duplicates99 <- CY99$UNITID[duplicated(CY99$UNITID)]

View(duplicates99)

duplicates13 <- CY13$UNITID[duplicated(CY13$UNITID)]

View(duplicates13)


#The datasets contain no duplicate entries for the UNITID variable. Since each observation 
#within the datasets will have a unique UNITID value, this variable will used for comparison. 
#Here, observing the number of values for the UNITID variable in CY99 which are not found in CY17. 
differences17 <- CY99$UNITID[!CY99$UNITID %in% CY17$UNITID]
#Count the missing observations.
length(differences17)

#Let's repeat this process and evaluate all remaining CY datasets against CY99. Recall that the value of 
#differencesXX is the number of observations in CY99 which are not found in CYXX.  
differences16 <- CY99$UNITID[!CY99$UNITID %in% CY16$UNITID]
length(differences16)

differences15 <- CY99$UNITID[!CY99$UNITID %in% CY15$UNITID]
length(differences15)

differences14 <- CY99$UNITID[!CY99$UNITID %in% CY14$UNITID]
length(differences14)

differences13 <- CY99$UNITID[!CY99$UNITID %in% CY13$UNITID]
length(differences13)

differences12 <- CY99$UNITID[!CY99$UNITID %in% CY12$UNITID]
length(differences12)

differences11 <- CY99$UNITID[!CY99$UNITID %in% CY11$UNITID]
length(differences11)

differences10 <- CY99$UNITID[!CY99$UNITID %in% CY10$UNITID]
length(differences10)

differences09 <- CY99$UNITID[!CY99$UNITID %in% CY09$UNITID]
length(differences09)

differences08 <- CY99$UNITID[!CY99$UNITID %in% CY08$UNITID]
length(differences08)

differences07 <- CY99$UNITID[!CY99$UNITID %in% CY07$UNITID]
length(differences07)

differences06 <- CY99$UNITID[!CY99$UNITID %in% CY06$UNITID]
length(differences06)

differences05 <- CY99$UNITID[!CY99$UNITID %in% CY05$UNITID]
length(differences05)

differences04 <- CY99$UNITID[!CY99$UNITID %in% CY04$UNITID]
length(differences04)

differences03 <- CY99$UNITID[!CY99$UNITID %in% CY03$UNITID]
length(differences03)

differences02 <- CY99$UNITID[!CY99$UNITID %in% CY02$UNITID]
length(differences02)

differences01 <- CY99$UNITID[!CY99$UNITID %in% CY01$UNITID]
length(differences01)

differences00 <- CY99$UNITID[!CY99$UNITID %in% CY00$UNITID]
length(differences00)

```
Although we need to evaluate each of the datasets to find the 'master' dataset--whose observations are most present in all other datasets--it is obvious that performing the process above will not be efficient.  Therefore, we will implement the process using a 'for' loop.  The 'for' loop will select a dataset and evaluate all others against it using the same logic as seen above.  The results will be stored in two separate lists.  

The first list, 'outcomes' will contain data in the form: 
      
      '<name of candidate dataset>:<number of observations in candidate dataset which are not in evaluated dataset>'
      
The second list, 'mean_outcomes' will store the values which will be later used to perform a 'summary(...)' of the observations.  The 'mean_outcomes' data will also be referenced later when we calculate the mean and standard deviation of observations across all dataset evaluations.  

```{r}       
#Create a list of the datasets. 
datasets <- list(CY99, CY00, CY01, CY02, CY03, CY04, CY05, CY06, CY07, CY08, CY09, CY10, CY11, CY12, CY13, CY14, CY15, CY16, CY17)

#Create a list of the dataset names, which will be used to populate the first element in the 'results' array for each dataset evaluated
dataset_names <- c("CY99", "CY00", "CY01", "CY02", "CY03", "CY04", "CY05", "CY06", "CY07", "CY08", "CY09", "CY10", "CY11", "CY12", "CY13", "CY14", "CY15", "CY16", "CY17")


#Create an array to store the final outcome, which will be a list of lists.  Each list in the 'outcome' array will contain as it's
#first element the name of the CYXX year that all other datasets are being compared against, and subsequent elements will contain
#the number of observations in the comparison year which are not found in the other CY datasets. A new element (list) is added to 
#'outcomes' at each iteration of 'i'.  
outcomes <- list(1)

#Create an array to store only numeric values of observations found so that arithmentic functions can later be performed on the data
mean_outcomes <- list(1)

#Create an array for intermediate use within the nested 'for' loop called 'results'.  The results array, at each iteration of
#of 'j', will be updated to include the number of observations in the jth dataset which are not in the ith dataset being evaluated. 
#Each new result will be stored as a new element in 'results' in the form:  "CY<XX>:<number of observations>"
results <- c("")

#Create an array to be used for calculating mean and/or summing of total observations present in ith, but absent in jth datasets.  
mean_results <- list(1)

#For each dataset from CY99 to CY17
N <- length(datasets)
for(i in 1:N){
  #Populate the first element of 'results' with the name of the dataset CY which all others are being compared against. 
  results[[1]] <- dataset_names[[i]]
  for(j in 1:N){
    differences <- datasets[[i]]$UNITID[!datasets[[i]]$UNITID %in% datasets[[j]]$UNITID]
    #differences <- dataset_names[[i]]$UNITID[!dataset_names[[i]]$UNITID %in% dataset_names[[j]]$UNITID]
    results[[j+1]] <- paste(dataset_names[[j]], length(differences), sep = ":", collapse = NULL)
    mean_results[[j]] <- length(differences)
    #After evaluating all datasets against the current ith dataset, push child arrays/lists to their parent array/list
    if(j == N){
      mean_outcomes[[i]] <- mean_results
      outcomes[[i]] <- results
     
    }#if
    
  }#for

}#for

outcomes
View(mean_outcomes)
#An example of the data stored at each element.  We'll display the last element in both lists, CY17.
View(outcomes[[19]])
View(mean_outcomes[[19]])

names(mean_outcomes)
#There are no names for each of the columns of data in the 'mean_outcomes' list object.  Let's add some
#from the 'dataset_names' array. 
names(mean_outcomes) <- dataset_names
names(mean_outcomes)

#We'll need to convert the column data in the 'mean_outcomes' list object to type 'integer' using a 'for' loop. 
for(i in 1:N){
  mean_outcomes[[i]] <- as.integer(mean_outcomes[[i]])
}

#Now that the 'mean_outcomes' list object contains names for each of it's contained lists, and the values of
#each list have been converted to integers, we must further convert the 'mean_outcomes' list object to a tibble.
mean_outcomes <- as_tibble(mean_outcomes)

```
Now that all datasets have been evaluated, it's time to analyze the results.  We will then 
be able to determine which of the datasets can be considered the 'master' dataset to which all others will be joined.  
As seen in the 'cohort' sheet within the 'data dictionary' document which accompanies the datasets, values for variables have been recorded for specific years only.  This information will be useful later, as we can later modify the above 'for' loop to evaluate the best 'master' dataset candidates for specific year spans, rather than for all years (CY99 - CY17) for which sufficient data has been recorded.  **Note: Since CY98 contains only 1,951 observations, and the next highest number of observations--found in CY99--is 6,466 (a difference of 30%).  Therefore CY96, CY97, and CY98 datasets have been excluded from evaluation for continutity purposes** 

```{r}
#Let's view the summary of mean_outcomes, which will show us the "observation" quantity data we've been seeking.
summary(mean_outcomes)
```

This data tells us quite a lot.   Let's create some graphs for the mean and standard deviation of missing observations so we can observe another representation of our evaluation results. 

```{r}
#Create a list object to store the mean number of obserations found in candidate CV dataset X which were not found in evaluated dataset Y, as recorded in 'mean_outcomes'. Then determine the average amount that the number of missing observations for each CY varies from the average number of missing observations for all years evaluted against the candidate CY dataset (standard deviation from the mean).  The 'meanValues' and 'stDevValues' lists will be used to build a graph which will allow us to visualize the magnitude and variance of observations present in each candidate year, which are missing from each evaluated year.

meanValues <- list(1)
stDevValues <- list(1)
for(i in 1:N){
  meanValues[[i]] <- as.integer(mean(mean_outcomes[[i]]))
  stDevValues[[i]] <- as.integer(sd(mean_outcomes[[i]]))
}
names(meanValues) <- dataset_names
names(stDevValues) <- dataset_names

meanValues <- as_tibble(meanValues)
stDevValues <- as_tibble(stDevValues)

View(meanValues)
View(stDevValues)

#Convert the 'meanValues' tibble data from wide to long using 'gather()'. 
meanValues <- gather(meanValues, Calendar_Year, Mean, CY99:CY17)
View(meanValues)

#Convert the 'stDevValues' tibble data from wide to long using 'gather()'. 
stDevValues <- gather(stDevValues, Calendar_Year, Standard_Deviation_From_Mean, CY99:CY17)
View(stDevValues)

#Make a historgram for 'meanValues' and another for 'stDevValues'. First, import ggplot.
library("ggplot2")
```

```{r}
ggplot(meanValues, aes(x = Calendar_Year, y = Mean)) + geom_boxplot()

```

```{r}
ggplot(stDevValues, aes(x = Calendar_Year, y = Standard_Deviation_From_Mean)) + geom_boxplot()

```
We can see that the dataset 'CY04' contains the greatest number of observations which are also found among all other datasets.  This is because the mean number of 'CY04' dataset observations missing from other datasets is the lowest among all other datasets, with a mean value of 716.6.  Additionally, the number of 'CY04' observations missing from each dataset varies from the mean number of observations less so than seen for other candidate datasets, at 453.  This means that 68% of the datasets evaluated against 'CY04' are missing less than or equal to 453 observations.  

Now that we've identified our 'master' dataset it's time to join all other datasets to 'CY04' where matching 'OPEID' values are present.  But before we attempt to join datasets, it is necessary to ensure that the data types for the variables (columns) of each dataset are correct and consistent across all datasets.      
```{r}
#For iteration, we need to have a 'dataTypes' atomic vector.  This will allow us to use them during 'for' loop iteration in which the data types of all columns in each dataset will be converted to their appropriate data types.  
dataTypes <- c("")

#Read in the data types.  From the 'CollegeScorecardDataDictionary', I have isolated the data types for each of the columns and stored in a .CSV file "data_typesWITH'as.'.csv".    

dataTypesImport <- as.list(read.csv("data_typesWITH'as.'.csv", header= FALSE, na.strings= c("NULL")))
dataTypesImport$V1 <- as.character(dataTypesImport$V1)
for(i in 1:length(dataTypesImport[[1]])){
  dataTypes[[i]] <- dataTypesImport$V1[[i]]
}

#Create a function which will change the data type for a column in a dataset and return the updated column.
changeDatatype <- function(j, dataset, datatype){
  if(datatype[[j]] == "as.double"){
    dataset[[j]] <- as.double(dataset[[j]])
  }
  else if(datatype[[j]] == "as.integer"){
    dataset[[j]] <- as.integer(dataset[[j]])
  }
  else if(datatype[[j]] == "as.character"){
    dataset[[j]] <- as.character(dataset[[j]])
  }
  return(dataset[[j]])
}

#For each dataset stored in 'datasets', change the data type for each of it's columns to the correct type indicated in the data dictionary which accompanies the datasets. 
for(i in 1:length(datasets)){
  CY <- datasets[[i]]
  for(j in 1:length(dataTypes)){
    CY[[j]] <- changeDatatype(j, datasets[[i]], dataTypes)
  }
  datasets[[i]] <- CY
}

```
Now all datasets should be updated, with the updated versions stored as elements in the 'datasets' list.  Let's verify that the data types for the columns have been updated by comparing one of the datasets in the 'datasets' list with it's original counterpart.

The CY99 dataset...
```{r}
#The original version...
head(CY99)
```
```{r}
#The updated version...
head(datasets[[1]])
```
We see that the changes have been made.  At this time it's a good idea to make sure we store the modified datasets back into their original scalar object names.  Then, we'll have the datasets stored in list and scalar formats for future use.  
```{r}
CY99 <- datasets[[1]]
CY00 <- datasets[[2]]
CY01 <- datasets[[3]]
CY02 <- datasets[[4]]
CY03 <- datasets[[5]]
CY04 <- datasets[[6]]
CY05 <- datasets[[7]]
CY06 <- datasets[[8]]
CY07 <- datasets[[9]]
CY08 <- datasets[[10]]
CY09 <- datasets[[11]]
CY10 <- datasets[[12]]
CY11 <- datasets[[13]]
CY12 <- datasets[[14]]
CY13 <- datasets[[15]]
CY14 <- datasets[[16]]
CY15 <- datasets[[17]]
CY16 <- datasets[[18]]
CY17 <- datasets[[19]]

```
Now, let's try to join each of the datasets at their intersections, starting with CY04. 
```{r}
#Perform a join, consisting of the intersection of CY04 and CY99. 
#####testMaster <- semi_join(CY99, CY04, by = c("OPEID"))
#####testMaster <- subset(CY04, CY04$OPEID == CY99$OPEID)
#####testMaster <- filter_all(CY99, any_vars(CY99$OPEID %in% CY04$OPEID))

#Since we're finding issues with building a dataframe which contains only the rows/observations whose OPEID value is present in our 'master' CY dataset, let's take another approach and 'rbind' all of the datasets.  This will combine all rows from all datasets.  After combining, we can filter out any rows/observations from CY dataset years whose OPEID value is not found in our 'master' CY dataset, CY04. 
combinedDatasets <- rbind(CY99, CY00, CY01, CY02, CY03, CY04, CY05, CY06, CY07, CY08, CY09, CY10, CY11, CY12, CY13, CY14, CY15, CY16, CY17)

View(combinedDatasets)

#Let's try to filter out any observations which are not present for all dataset years.  
refinedCombinedDatsets <- filter()

```
